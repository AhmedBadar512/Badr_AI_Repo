{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convbnrelu(K.layers.Layer):\n",
    "    def __init__(self, filters, kernels=(3, 3), strides=1, padding=\"SAME\", data_format=\"NHWC\", dilations=1, use_bias=False):\n",
    "        super(convbnrelu, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernels = kernels\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.dilations = dilations\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        if self.data_format == 'NHWC':\n",
    "            channel_index = -1\n",
    "            data_format_keras = 'channels_last'\n",
    "        elif self.data_format == 'NCHW':\n",
    "            channel_index = 1\n",
    "            data_format_keras = 'channels_first'\n",
    "        self.bn = K.layers.BatchNormalization(axis=channel_index)\n",
    "        self.relu = K.layers.ReLU()\n",
    "        self.conv = K.layers.Conv2D(filters=self.filters, \n",
    "                                    kernel_size=self.kernels, \n",
    "                                    strides=self.strides, \n",
    "                                    padding=self.padding.lower(), \n",
    "                                    data_format=data_format_keras, \n",
    "                                    use_bias=False)\n",
    "    def call(self, inputs):\n",
    "        tensor = self.conv(inputs)\n",
    "        tensor = self.bn(tensor)\n",
    "        tensor = self.relu(tensor)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dwconvbnrelu(K.layers.Layer):\n",
    "    def __init__(self, depth_multiplier=1, kernels=(3, 3), strides=1, padding=\"SAME\", data_format=\"NHWC\", dilations=1, use_bias=False):\n",
    "        super(dwconvbnrelu, self).__init__()\n",
    "        self.depth_multiplier = depth_multiplier\n",
    "        self.kernels = kernels\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.dilations = dilations\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        if self.data_format == 'NHWC':\n",
    "            channel_index = -1\n",
    "            data_format_keras = 'channels_last'\n",
    "        elif self.data_format == 'NCHW':\n",
    "            channel_index = 1\n",
    "            data_format_keras = 'channels_first'\n",
    "        self.bn = K.layers.BatchNormalization(axis=channel_index)\n",
    "        self.relu = K.layers.ReLU()\n",
    "        self.conv = K.layers.DepthwiseConv2D(kernel_size=self.kernels, \n",
    "                                             strides=self.strides, \n",
    "                                             padding=self.padding.lower(), \n",
    "                                             data_format=data_format_keras, \n",
    "                                             depth_multiplier=self.depth_multiplier,\n",
    "                                             use_bias=False)\n",
    "    def call(self, inputs):\n",
    "        tensor = self.conv(inputs)\n",
    "        tensor = self.bn(tensor)\n",
    "        tensor = self.relu(tensor)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combconv(K.layers.Layer):\n",
    "    def __init__(self, filters, kernels=(3, 3), strides=1, padding=\"SAME\", data_format=\"NHWC\", dilations=1, use_bias=False):\n",
    "        super(combconv, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernels = kernels\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.data_format = data_format\n",
    "        self.dilations = dilations\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "        if self.data_format == 'NHWC':\n",
    "            channel_index = -1\n",
    "            data_format_keras = 'channels_last'\n",
    "        elif self.data_format == 'NCHW':\n",
    "            channel_index = 1\n",
    "            data_format_keras = 'channels_first'\n",
    "        self.dw = dwconvbnrelu(kernels=self.kernels, strides=self.strides, padding=self.padding, data_format=self.data_format)\n",
    "        self.pw = convbnrelu(self.filters, kernels=(1, 1), padding=self.padding, data_format=self.data_format)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        tensor = self.pw(inputs)\n",
    "        tensor = self.dw(tensor)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hardblock(K.Model):\n",
    "    def get_link(self, layer, base_ch, growth_rate, grmul):\n",
    "        if layer == 0:\n",
    "            return base_ch, []\n",
    "        out_channels = growth_rate\n",
    "        link = []\n",
    "        for i in range(10):\n",
    "            dv = 2 ** i\n",
    "            if layer % dv == 0:\n",
    "                k = layer - dv\n",
    "                link.append(k)\n",
    "                if i > 0:\n",
    "                    out_channels *= grmul\n",
    "        out_channels = int(int(out_channels + 1) / 2) * 2\n",
    "        return out_channels, link\n",
    "    \n",
    "    def __init__(self, base_ch, growth_rate, grmul, n_layers, dwconv=True, keepBase=False):\n",
    "        super(hardblock, self).__init__()\n",
    "        self.links = []\n",
    "        layers_ = []\n",
    "        self.keepBase = keepBase\n",
    "        self.out_channels = 0\n",
    "        for i in range(1, n_layers + 1):\n",
    "            outch, link = self.get_link(i, base_ch, growth_rate, grmul)\n",
    "            self.links.append(link)\n",
    "            if dwconv:\n",
    "                layers_.append(combconv(outch))\n",
    "            else:\n",
    "                layers_.append(convbnrelu(outch))\n",
    "\n",
    "            if (i % 2 == 0) or (i == n_layers - 1):\n",
    "                self.out_channels += outch\n",
    "        self.layers_list = layers_\n",
    "        self.concatenate = K.layers.Concatenate(axis=-1)\n",
    "    \n",
    "    def get_out_ch(self):\n",
    "        return self.out_channels\n",
    "        \n",
    "    def call(self, x):\n",
    "        layers_ = [x]\n",
    "        \n",
    "        for layer in range(len(self.layers_list)):\n",
    "            link = self.links[layer]\n",
    "            tin = []\n",
    "            for i in link:\n",
    "                tin.append(layers_[i])\n",
    "            if len(tin) > 1:\n",
    "                x = self.concatenate(tin)\n",
    "            else:\n",
    "                x = tin[0]\n",
    "            out = self.layers_list[layer](x)\n",
    "            layers_.append(out)\n",
    "            \n",
    "        t = len(layers_)\n",
    "        out_ = []\n",
    "        for i in range(t):\n",
    "          if (i == 0 and self.keepBase) or \\\n",
    "             (i == t-1) or (i%2 == 1):\n",
    "              out_.append(layers_[i])\n",
    "        out = self.concatenate(out_)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_layer = combconv(512, data_format='NHWC')\n",
    "# x = K.layers.Input(shape=(3, 512, 512))|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_hblock = hardblock(3, 1.7, 2, 12, keepBase=False, dwconv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = K.Model(inputs=x, outputs=my_hblock(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HarDNet(K.Model):\n",
    "    def __init__(self, depth_wise=False, arch=85, pretrained=True, num_classes=10, weight_path=''):\n",
    "        super().__init__()\n",
    "        first_ch  = [32, 64]\n",
    "        second_kernel = 3\n",
    "        max_pool = True\n",
    "        grmul = 1.7\n",
    "        drop_rate = 0.1\n",
    "        \n",
    "        #HarDNet68\n",
    "        ch_list = [  128, 256, 320, 640, 1024]\n",
    "        gr       = [  14, 16, 20, 40,160]\n",
    "        n_layers = [   8, 16, 16, 16,  4]\n",
    "        downSamp = [   1,  0,  1,  1,  0]\n",
    "        \n",
    "        if arch==85:\n",
    "            #HarDNet85\n",
    "            first_ch  = [48, 96]\n",
    "            ch_list = [  192, 256, 320, 480, 720, 1280]\n",
    "            gr       = [  24,  24,  28,  36,  48, 256]\n",
    "            n_layers = [   8,  16,  16,  16,  16,   4]\n",
    "            downSamp = [   1,   0,   1,   0,   1,   0]\n",
    "            drop_rate = 0.2\n",
    "        elif arch==39:\n",
    "            #HarDNet39\n",
    "            first_ch  = [24, 48]\n",
    "            ch_list = [  96, 320, 640, 1024]\n",
    "            grmul = 1.6\n",
    "            gr       = [  16,  20, 64, 160]\n",
    "            n_layers = [   4,  16,  8,   4]\n",
    "            downSamp = [   1,   1,  1,   0]\n",
    "          \n",
    "        if depth_wise:\n",
    "            second_kernel = 1\n",
    "            max_pool = False\n",
    "            drop_rate = 0.05\n",
    "        \n",
    "        blks = len(n_layers)\n",
    "        self.base = []\n",
    "\n",
    "        # First Layer: Standard Conv3x3, Stride=2\n",
    "        self.base.append (\n",
    "             convbnrelu(filters=first_ch[0], kernels=3,\n",
    "                       strides=2) )\n",
    "  \n",
    "        # Second Layer\n",
    "        self.base.append ( convbnrelu(first_ch[1],  kernels=second_kernel) )\n",
    "        \n",
    "        # Maxpooling or DWConv3x3 downsampling\n",
    "        if max_pool:\n",
    "            self.base.append(K.layers.MaxPool2D(pool_size=(3, 3), strides=2, padding='same', data_format='channels_last'))\n",
    "        else:\n",
    "            self.base.append ( dwconvbnrelu(strides=2) )\n",
    "\n",
    "        # Build all HarDNet blocks\n",
    "        ch = first_ch[1]\n",
    "        for i in range(blks):\n",
    "            blk = hardblock(ch, gr[i], grmul, n_layers[i], dwconv=depth_wise)\n",
    "            ch = blk.get_out_ch()\n",
    "            self.base.append ( blk )\n",
    "            \n",
    "            if i == blks-1 and arch == 85:\n",
    "                self.base.append(K.layers.Dropout(0.1))\n",
    "            \n",
    "            self.base.append ( convbnrelu(ch_list[i], kernels=1) )\n",
    "            ch = ch_list[i]\n",
    "            if downSamp[i] == 1:\n",
    "                if max_pool:\n",
    "                    self.base.append(K.layers.MaxPool2D(pool_size=(2, 2), strides=2, padding='same', data_format='channels_last'))\n",
    "                else:\n",
    "                    self.base.append (dwconvbnrelu(strides=2))\n",
    "        self.base.append(K.layers.GlobalAveragePooling2D('channels_last'))\n",
    "        self.base.append(K.layers.Flatten())\n",
    "        self.base.append(K.layers.Dropout(drop_rate))\n",
    "        self.base.append(K.layers.Dense(1000))\n",
    "        self.base.append(K.layers.Dense(num_classes))\n",
    "          \n",
    "    def call(self, x):\n",
    "        y = x\n",
    "        for layer in self.base:\n",
    "            y = layer(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = K.applications.resnet_v2.ResNet50V2(weights=None, classes=10, input_shape=(28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()\n",
    "model = HarDNet(arch=39)\n",
    "# model.build((None, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "def get_loss(logits, labels):\n",
    "    probs = K.activations.softmax(logits, axis=-1)\n",
    "    loss = K.losses.CategoricalCrossentropy()(probs, labels)\n",
    "    return loss\n",
    "\n",
    "def train_step(input_imgs, labels, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(input_imgs)\n",
    "#         loss = tf.losses.categorical_crossentropy(logits, labels, from_logits=True)\n",
    "        loss = tf.reduce_mean(K.losses.categorical_crossentropy(labels, logits, from_logits=True))\n",
    "#         loss = get_loss(logits, labels)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = tfds.load(\"fashion_mnist\", split=\"train\", with_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_test = tfds.load(\"fashion_mnist\", split=\"test\", with_info=False).shuffle(1000).batch(24, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(model, test_data):\n",
    "    print(\"Calculating Validation Accuracy...\")\n",
    "    accuracy = K.metrics.Accuracy()\n",
    "    for test_batch in test_data:\n",
    "        pred = tf.argmax(model(test_batch['image']/1), axis=-1)\n",
    "        lab = test_batch['label']\n",
    "        accuracy.update_state(lab, pred)\n",
    "    print(\"Accuracy: \", accuracy.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_metrics(model, fmnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmnist_test = fmnist_test.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, mini_batch in enumerate(fmnist_test):\n",
    "#     val_loss += get_loss(model(mini_batch['image']/1), tf.one_hot(mini_batch['label'], 10))\n",
    "# print(\"Validation Loss: \", val_loss/(n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist_new = fmnist.shuffle(1024).batch(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2499\n"
     ]
    }
   ],
   "source": [
    "for n, mini_batch in enumerate(zip(fmnist_new, fmnist_test.repeat())):\n",
    "    pass\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = \"/home/badar/Code/Badr_AI_Repo/logs\"\n",
    "epochs = 5\n",
    "batch_size = 24\n",
    "n_classes = 10\n",
    "total_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.create_file_writer(logdir)\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 17664/60000, Loss: 0.5811262726783752 Val Loss: 15.388900756835938\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for step, (mini_batch, val_mini_batch) in enumerate(zip(fmnist_new, fmnist_test.repeat().shuffle(1024, reshuffle_each_iteration=True))):\n",
    "        loss = train_step(tf.cast(mini_batch['image'], tf.float32), tf.one_hot(mini_batch['label'], 10), model, optimizer)\n",
    "        val_loss = get_loss(model(val_mini_batch['image']/1), tf.one_hot(mini_batch['label'], 10))\n",
    "        print(\"Epoch {}: {}/{}, Loss: {} Val Loss: {}\".format(epoch, step*batch_size, 60000, loss.numpy(), val_loss.numpy()))\n",
    "        tf.summary.scalar(\"loss\", loss, step=total_steps+step)\n",
    "        tf.summary.scalar(\"val_loss\", val_loss, step=total_steps+step)\n",
    "        clear_output(wait=True)\n",
    "    total_steps += (step + 1)\n",
    "#     val_loss = 0\n",
    "#     print(\"Calculating Val Loss...\")\n",
    "#     for n, mini_batch in enumerate(fmnist_test):\n",
    "#         val_loss += get_loss(model(mini_batch['image']/1), tf.one_hot(mini_batch['label'], 10))\n",
    "#     print(\"Validation Loss: \", (val_loss/(n+1)).numpy())\n",
    "#     calc_metrics(model, fmnist_test)\n",
    "#     fmnist_new = fmnist_new.shuffle(1000)\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8 0 8 5 8 1 9 4 1 9 7 1 3 9 4 7 1 9 5 0 2 7 5 7], shape=(24,), dtype=int64)\n",
      "tf.Tensor([8 0 1 5 8 1 9 3 1 9 7 1 3 9 4 7 1 9 5 0 2 7 5 9], shape=(24,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x in fmnist_new.take(1):\n",
    "    print(tf.argmax(model(x['image']/1), axis=-1))\n",
    "    print(x['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
